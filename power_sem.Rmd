---
title: "Power Analysis for Multivariate Analysis Using SEM"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(lavaan) 
```

# Power Analysis in SEM

Author: jeremy.miles@gmail.com
Date: 2020-12-05

This analysis updates the paper [A framework for power analysis using a 
structural equation modelling procedure](https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/1471-2288-3-27)
(Open access paper.)

The paper describes how to do power analysis for multivariate analysis using
SEM.
The examples in the paper are written using Mx. This file updates the programs
and gives some examples using R and the lavaan package. 

The approach relies on the fact that many analyses are special cases of SEMs,
and SEMs are pretty straightforward to calculate power for, using the
[Satorra-Saris method](https://link.springer.com/article/10.1007/BF02294150).

Note that I'm going to assume that you know how to use lavaan (and R) for this
tutorial and won't give long explanations of models (or of power).

# Examples

The basic approach is always the same. Set up the data which matches the
population that we are interested in, by making a model in lavaan that is
saturated. Then get the implied means and covariance matrix (or matrices). Then
fit the data to the model with the parameter of interest constrained to zero.

The chi-square from this model with the parameter constrained to zero is the
non-centrality parameter. 


## t test
We'll start with some simple examples, like a t-test. 

### t test in SEM

There are a couple of ways to do a t-test in SEM. We can use a regression
approach, or a multiple groups approach. 

We'll show these by generating some data, and doing a t-test.

First we'll make up some data. We'll generate two groups, with 100 in each
group; group 0 will have a population mean of 0, and group 1 will have a
population mean of 0.2. Both groups will have sd = 1.

```{r t1}

set.seed(1234)
d1 <- data.frame(x = c(rep(0, 100), rep(1, 100)),
                y = rnorm(200)) %>%
  dplyr::mutate(y = y + 0.2 * x)

```
Now we'll do a t-test.


```{r t2}

t.test(y ~ x, data = d1, var.equal = TRUE)

```

Now we'll do it with SEM, using both the regression method and the multiple
groups method.

First, we'll use the regression model:

```{r t3}

sem_reg_model <- "
  y ~ x
  x ~~ x
  y ~~ y"
sem_reg_fit <- lavaan::sem(sem_reg_model, data = d1)
summary(sem_reg_fit, ci = TRUE)
```


Second, the multiple group approach.

```{r t4}

sem_mg_model <- "
  y ~ c(ma, mb) * 1
  y ~~ y
  d1 := ma - mb
"
sem_mg_fit <- lavaan::sem(sem_mg_model, data = d1, group = "x")
summary(sem_mg_fit, ci = TRUE)
```

Cool! Same results (to the third decimal place.)

We'll cover an extra little detail. The t-test is often said to assume
homogeneity of variance - this assumption only really matters if your sample
sizes are different in the two groups, however the t-test, by default, corrects
for homogeneity of variance - although you can turn this off. 

We'll generate some data which violates homogeneity of variance.

```{r t5}
set.seed(321)
d2 <- data.frame(x = c(rep(0, 100), rep(1, 200)),
                 y = rnorm(300)) %>%
  dplyr::mutate(y = ifelse(x == 1, y = y * 2 + 0.2, y))

t.test(y ~ x, data = d2, var.equal = FALSE)
t.test(y ~ x, data = d2, var.equal = TRUE)

```

The result is statistically significant, if we don't assume homogeneity of variance, but is significant if we do.

We can relax the homogeneity of variance assumption in the multiple groups test by allowing the variances to differ across groups. 

```{r t6}

sem_mg_model_hov <- "
  y ~ c(ma, mb) * 1
  y ~~  c(va, vb) * y
  d := ma - mb
  va == vb
"

sem_mg_model_no_hov <- "
  y ~ c(ma, mb) * 1
  y ~~ c(va, vb) *  y
  d := ma - mb
"

sem_mg_fit_hov <- lavaan::sem(sem_mg_model_hov, data = d2, group = "x")
summary(sem_mg_fit_hov, ci = TRUE)
sem_mg_fit_no_hov <- lavaan::sem(sem_mg_model_no_hov, data = d2, group = "x")
summary(sem_mg_fit_no_hov, ci = TRUE)
```

We get the same results (and the same difference in statistical significance).

However, we can relax the homogeneity of variance assumption by using a robust
estimator (e.g. Satorra-Bentler Chi-Square and related; also called a sandwich
estimator). In SEM, we think about robust estimates as being about
distributional assumptions, but they also relax the homogeneity of variance
assumption. 

```{r t7}

sem_reg_model <- "
  y ~ x
  y ~~ y
  "
sem_reg_hov <- lavaan::sem(sem_reg_model, data = d2)
sem_reg_no_hov <- lavaan::sem(sem_reg_model, data = d2, 
                              estimator = "MLR")
summary(sem_reg_hov)
summary(sem_reg_no_hov)


```
Same result! Whoo hoo!

Finally, we don't need the raw data to calculate an SEM. We can do it with the
summary data. For a multiple group model, all we need is the mean of the two
groups, the (co)variance(s) of the two groups, and the number in each group. 

Each of these needs to be in a list, with two items. The variances need to be in
a matrix. We show how to create the summary data below. (If you have the data,
you wouldn't do this, but if you only have the summary data we'll see later that
you can just type the values in.)

```{r t8}

df_summary <- d2 %>%
  dplyr::group_by(x) %>%
  dplyr::summarise(m = mean(y),
                   n = n(),
                   var = var(y)) 
list_means <- list(df_summary$m[[1]], df_summary$m[[2]])
list_ns <- list(df_summary$n[[1]], df_summary$n[[2]])
list_vars <- list(matrix(df_summary$var[[1]]),
                  matrix(df_summary$var[[2]]))
# name rows and columns for each variable
list_vars <- lapply(list_vars, function(mat) {
  rownames(mat) <- colnames(mat) <- "y"
  return(mat)
})

list_means
list_ns
list_vars
```

Now we can fit the same models as before, but using summary data. All we change
is the data, we don't change the models. 

```{r t9}
sem_mg_fit_hov_summary <- 
  lavaan::sem(sem_mg_model_hov,
              sample.mean = list_means,
              sample.cov = list_vars,
              sample.nobs = list_ns)
sem_mg_fit_no_hov_summary <-
  lavaan::sem(sem_mg_model_no_hov,
              sample.mean = list_means,
              sample.cov = list_vars,
              sample.nobs = list_ns)
summary(sem_mg_fit_hov_summary)
summary(sem_mg_fit_no_hov_summary)

```

## Power 

OK, now we've established that we can do t-tests with SEM. Why do we care?

We care because calculating power for SEMs can be a lot easier (and quicker)
than calculating power using other methods. 

First we'll calculate power for a t-test using the power.t.test() function, for
d = 0.2, n = 100 (per group).

```{r t10}

power.t.test(d = 0.2, n = 100)

```

The Satorra-Saris method makes it easy to get power from SEM. 
First, set up your data that contains effects of the magnitud you want to test.
We'll use the multiple groups approach, and instead of calculating the means,
variances, etc from the data, we'll just type them in.

Means are 0 and 0.2, variances are 1.00, and ns are 100

```{r t11}

mat <- matrix(1)
rownames(mat) <- "y"
colnames(mat) <- "y"
covs <- list(mat, mat)
means <- list(0, 0.2)
ns <- list(100, 100)

```



Now we have the population data, we can fit the null model to it. We free all of
the parameters, except the single one that we are interested in. Then we fit
that model. 

We then get a chi-square for the fit. The probability tells us the probability
of obtaining the effect size (or larger) given that the null hypothesis is
false. We know the null hypothesis is not false, because we generated the data. 



```{r t12}
model_mg_1_null <- "
  y ~~ y
  y ~  c(a, a) * 1  "
fit_mg_1_null <- lavaan::sem(
  model_mg_1_null, 
  sample.cov = covs, 
  sample.mean = means,
  sample.nobs = ns)
summary(fit_mg_1_null)
```


However, this is our _population_chi-square, which we're sampling from. For
statistical significance testing, we ask what the probability of obtaining a
result of whatever magnitude we found is, given that the null hypothesis is
false.  

We don't want to know that. We want to know the probability of obtaining a
result of statistically significant magnitude, given that the null hypothesis is
false, and the population data look like the population as we defined it. 

This population value of chi-square is called the non-centrality parameter
(ncp). 

We want to know the probability of getting a significant result, given our ncp.
So we need to know what the critical value of chi-square is. To find this we use
the qchisq() function. 

Then we use the pchisq() function to find the power. 

```{r t13}
# Find the chi-square for the population, this is ncp.
ncp <- fitMeasures(fit_mg_1_null)[["chisq"]]
## ncp
ncp 
## Critical Chi-Square
crit_chi <- qchisq(p = 0.95, df = 1)
## Power
1 - pchisq(q = crit_chi, df = 1, ncp = ncp) 

```

Look at that. Same value of power. 


This was kind of complex, but the basic idea can be extended to any model that
you can represent in an SEM, and that includes anova, manova, repeated measures
manova, mancova, 2 way repeated measures anova, etc.  We can also, as we'll see,
relax some assumptions. 

A nice thing about this approach is that the non-centrality parameter is a
multiple of the sample size. If you double the sample size, you double the
non-centrality parameter. 

This is useful, because this approach tells us the power for a certain sample
size. Usually, I want to know the sample size that's required for a certain
power.

But how do I know how large I need my non-centrality parameter (ncp) to be?
Well, a useful rule of thumb is that power of 0.8 is obtained when the 
non-centrality parameter is approximately equal to the chi-square that gives a 
p-value of around 0.005.

So, first we find out what the ncp needs to be, then we find find how many times
larger (or smaller) this value is than our NCP, then we increase our sample size
by this value.

```{r t14}
# Find the desired ncp
desired_ncp <- qchisq(p = 0.995, df = 1)
## sample size multiplier
n_multiplier <- desired_ncp / ncp
n_multiplier
# we had ns already set up, so we can multiply this.
# But it needs to be an integer, so we round it.
new_ns <- lapply(ns, function(n) round(n_multiplier * n)) 
# See what the new sample sizes are
new_ns
model_mg_new_null <- "
  y ~~ y
  y ~  c(a, a) * 1  "
fit_mg_new_null <- lavaan::sem(
  model_mg_new_null, 
  sample.cov = covs, 
  sample.mean = means,
  sample.nobs = new_ns)

ncp <- fitMeasures(fit_mg_new_null)[["chisq"]]
## ncp
ncp 
## Critical Chi-Square
crit_chi <- qchisq(p = 0.95, df = 1)
## Power
1 - pchisq(q = crit_chi, df = 1, ncp = ncp) 

```

Finally, we'll see if that's correct.

```{r t15}

power.t.test(power = 0.8, d = 0.2)

```
It appears that we are out by 1. Which is certainly close enough for power
analysis.

### That was a lot of effort

It was. But we can do lots of clever things now. What happens if we think we
might have different variances or different sample sizes in each group? We can't
use the base power functions (for different sample sizes, the pwr package has
some useful functions.) We can run simulations, but these get to be hard work
when we want to run lots of different tests to explore our options.

For example, if we think the the variances will be (1, 4), the sample sizes
(100, 400) and the difference 0.2, we could run a simulation.

```{r t16}

n0 = 100
n1 = 400
v0 = 1
v1 = 4
diff = 0.2

set.seed(1234)
system.time({
  res <- lapply(seq(1e4), function(i) {
    d2 <- data.frame(x = c(rep(0, n0), rep(1, n1)),
                     y = rnorm(n0 + n1) * v0) %>%
       dplyr::mutate(
        y = ifelse(x == 1, (v1 * y) + 0.2, y))
    sig <- t.test(y ~ x, data = d2)$p.value < 0.05
    return(sig)
  }) %>%
    unlist() %>%
    mean()
})
## power
res

```
Hmm... that's not a lot of power. And it takes 42 seconds to run on my computer. 


```{r t17}
mat0 <- matrix(v0^2)
mat1 <- matrix(v1^2)
colnames(mat0) <- colnames(mat0) <- "y"
rownames(mat0) <- rownames(mat1) <- "y"
covs <- list(mat0, mat1)
means <- list(0, diff)
ns <- list(n0, n1)

model_mg_2 <- "
  y ~~ y
  y ~  c(a, a) * 1 "
fit_mg_2 <- lavaan::sem(
  model_mg_2, 
  sample.cov = covs, 
  sample.mean = means,
  sample.nobs = ns)

ncp <- fitMeasures(fit_mg_2)[["chisq"]]
## Critical Chi-Square
crit_chi <- qchisq(p = 0.95, df = 1)
## Power
1 - pchisq(q = crit_chi, df = 1, ncp = ncp) 

```
OK, let's see what sample size we need, and then test it to see if it's correct.

```{r t18}

# Find the desired ncp
desired_ncp <- qchisq(p = 0.995, df = 1)
## sample size multiplier
n_multiplier <- desired_ncp / ncp
n_multiplier

ns <- lapply(ns, function(n) return(n * n_multiplier))

# new ns
ns

model_mg_2 <- "
  y ~~ y
  y ~  c(a, a) * 1 "

fit_mg_2 <- lavaan::sem(
  model_mg_2, 
  sample.cov = covs, 
  sample.mean = means,
  sample.nobs = ns)

ncp <- fitMeasures(fit_mg_2)[["chisq"]]
## Critical Chi-Square
crit_chi <- qchisq(p = 0.95, df = 1)
## Power
1 - pchisq(q = crit_chi, df = 1, ncp = ncp) 



```

If you've got a simple power problem, it's quickest to use the base R functions.
If you've got a slightly more complex problem, it's quickest to run a simulation.

If you want to test 20 different sample sizes, with 20 different ratios of
variances, that's going to take almost 5 hours of computer time to run the
simulations - it's going to be quicker to wrap the above code into a loop, and
run that. 